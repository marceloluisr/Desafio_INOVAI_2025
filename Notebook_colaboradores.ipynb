{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b42ea9b-50db-4034-8104-b76fec15f39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<a target=\"_blank\" href=\"#\">\n",
       "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
       "</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"#\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c638bb-9143-4f06-826a-11cb685bfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c936fc-f914-43b5-8f13-b204251632df",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3d9f8-e717-4ad7-b790-57ef543dbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from skimage.io import imshow, imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfa732-c197-4a8a-998d-134cec1404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Define uma semente global para garantir reprodutibilidade em PyTorch, NumPy e random.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # para múltiplas GPUs\n",
    "\n",
    "    # Garante comportamento determinístico (pode impactar performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"Semente definida: {seed}\")\n",
    "set_torch_seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40374206-58d7-4f86-aa8b-b78271143684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c868833-5640-4387-826a-e23f962766a5",
   "metadata": {},
   "source": [
    "# Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae53dd-3da0-4000-9fa3-aff43d6c5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_image_files(root_dir, extensions={'.jpg', '.jpeg', '.png', '.bmp'}): # utils junior novo\n",
    "    \"\"\"\n",
    "    Lista todas as imagens recursivamente.\n",
    "    Args:\n",
    "      - root_dir: image root directory\n",
    "      - extensions: the extension\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if os.path.splitext(fname)[1].lower() in extensions:\n",
    "                image_paths.append(os.path.join(dirpath, fname))\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def criar_scaler_para_split(dataframe, dir_arquivo_saida):\n",
    "  \"\"\"\n",
    "  Criar um scaler baseado em Z-Score normalization a partir dos feature vector reais\n",
    "  associados com cada ID de amostra. O scaler é uma funçao que permite inversão para \n",
    "  permitir a volta para o valor original\n",
    "\n",
    "  Z-Score Nomralization: um tipo de padronização que substrai a média e divide pelo desvio padrão.\n",
    "\n",
    "  args:\n",
    "    dataframe: datataframe originado de medidas_dados_sinteticos.csv e associado com um split\n",
    "    nome_arquivo_saida: nome do arquivo no formato .pkl que contém objeto StandarScaler do sklearn.\n",
    "\n",
    "  \"\"\"\n",
    "  if \".pkl\" not in dir_arquivo_saida:\n",
    "    raise ValueError(\"Nome do arquivo deve conter a extensão .pkl\")\n",
    "\n",
    "  # Define colunas de rótulo\n",
    "  exclude_cols = ['id', 'height', 'split']\n",
    "  label_cols = [col for col in dataframe.columns if col not in exclude_cols]\n",
    "\n",
    "  # Extrai rótulos como matriz\n",
    "  labels_original = dataframe[label_cols].values.astype('float32')\n",
    "\n",
    "  # Fit e transformação\n",
    "  scaler = StandardScaler()\n",
    "  scaler = scaler.fit(labels_original)\n",
    "\n",
    "  # Salva o scaler para uso na inferência\n",
    "  joblib.dump(scaler, dir_arquivo_saida)\n",
    "\n",
    "\n",
    "def salvar_silhuetas(img, mask, output_folder, nome_imagem):\n",
    "  \"\"\"\n",
    "  Salva a máscara binária e a imagem com contornos da silhueta em arquivos PNG.\n",
    "\n",
    "  Esta função realiza os seguintes passos:\n",
    "  1. Salva a máscara binária da silhueta.\n",
    "  2. Detecta contornos na máscara limpa.\n",
    "  3. Desenha os contornos sobre a imagem original.\n",
    "  4. Salva a imagem com os contornos desenhados.\n",
    "\n",
    "  Args:\n",
    "  ----------\n",
    "  img : torch.Tensor\n",
    "      Tensor da imagem original com formato (1, C, H, W), normalizada entre 0 e 1.\n",
    "\n",
    "  mask : numpy.ndarray\n",
    "      Máscara binária da silhueta com valores entre 0 e 1.\n",
    "\n",
    "  output_folder : str\n",
    "      Caminho da pasta onde os arquivos serão salvos. A pasta será criada se não existir.\n",
    "\n",
    "  nome_imagem : str\n",
    "      Nome base para os arquivos gerados (sem extensão). Será usado para nomear os arquivos\n",
    "      de saída como \"<nome_imagem>_mask.png\" e \"<nome_imagem>_boundary.png\".\n",
    "\n",
    "  Returns:\n",
    "  -------\n",
    "  None\n",
    "      Os arquivos são salvos diretamente no sistema de arquivos. A função não retorna valores.\n",
    "  \"\"\"\n",
    "  # Garantir que a pasta de saída existe\n",
    "  os.makedirs(output_folder, exist_ok=True)\n",
    "  # Salvar a máscara binária\n",
    "  mask_path = os.path.join(output_folder, f\"{nome_imagem}_mask.png\")\n",
    "  cv2.imwrite(mask_path, mask * 255)\n",
    "  # Preparar imagem original\n",
    "  img_arr = img.cpu().squeeze().permute((1, 2, 0)).detach().numpy().copy()\n",
    "  img_arr = (img_arr * 255).astype(np.uint8)\n",
    "  \n",
    "  #mask_clean = (mask > 0.5).astype(np.uint8) * 255\n",
    "  #mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "  #mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "  mask = (mask > 0.5).astype(np.uint8) * 255\n",
    "  # Encontrar contornos\n",
    "  contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  # Desenhar contornos sobre a imagem original\n",
    "  overlay = img_arr.copy()\n",
    "  cv2.drawContours(overlay, contours, -1, (0, 0, 255), thickness=2)\n",
    "  # Salvar imagem com contornos\n",
    "  boundary_path = os.path.join(output_folder, f\"{nome_imagem}_boundary.png\")\n",
    "  cv2.imwrite(boundary_path, overlay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4361ed-2377-448c-9b30-8adb3cda297f",
   "metadata": {},
   "source": [
    "# Análise Exploratória e Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713c864-b73c-4fd9-a55b-f9975db9fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio_pasta_dados_drive = '/content/drive/MyDrive/Colab Notebooks/SelecaoINOVAI/datasets'\n",
    "\n",
    "# List files\n",
    "os.listdir(diretorio_pasta_dados_drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04d349-fa97-46a6-8de6-b8d5c3c3c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copia arquivo CSV do drive para o colab\n",
    "shutil.copy(diretorio_pasta_dados_drive + '/medidas_dados_sinteticos.csv', '/content/medidas_dados_sinteticos.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345645c-d117-4de3-840a-6725d66f59e7",
   "metadata": {},
   "source": [
    "O dataset contém imagens frontais e lateriais do corpo.\n",
    "- cada pasta dentro do dos arquivos zip possuem nomes que indicam o ID da amostra\n",
    "- Cada ID contém uma imagem frontal e outra lateral.\n",
    "- 'syn_m' na subpasta indica masculino\n",
    "- 'sys_f' na subpasta indica feminino\n",
    "- Cada ID da imagem possui informações de medidas do corpo no arquivo medidas_dados_sinteticos.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af83415-7359-4462-b73c-89d7d7ca47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unzip as pastas  'homens_15k.zip' e 'mulheres_15k.zip' onde as imagens estão contidas\n",
    "\n",
    "# Pasta de destino para salvar no colab [ALTERE AQUI se necessário], e será removido se desligar o ambiente\n",
    "destination_path = '/content/unzipped_files'\n",
    "\n",
    "zip_file_paths = [\n",
    "    diretorio_pasta_dados_drive + '/mulheres_15k.zip',\n",
    "    diretorio_pasta_dados_drive + '/homens_15k.zip'\n",
    "]\n",
    "\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Unzip\n",
    "for zip_file_path in zip_file_paths:\n",
    "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb2b273-783e-4c14-8c8e-1a3cb6452f64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## vamos observar o arquivos CSV de medidas corporais\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_medidas = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/content/medidas_dados_sinteticos.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_medidas.shape)\n\u001b[32m      4\u001b[39m df_medidas.tail()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## vamos observar o arquivos CSV de medidas corporais\n",
    "df_medidas = pd.read_csv(\"/content/medidas_dados_sinteticos.csv\")\n",
    "print(df_medidas.shape)\n",
    "df_medidas.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078dc95-c9bf-464d-9564-5f9543edecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renomeando algumas colunas para pt-br\n",
    "df_medidas.rename(columns={\n",
    "    'neck_circ': 'pescoco_circ',\n",
    "    'chest_circ': 'peito_circ',\n",
    "    'waist_circ': 'cintura_circ',\n",
    "    'hip_circ': 'quadril_circ',\n",
    "    'thigh_circ': 'coxa_circ',\n",
    "    'knee_circ': 'joelho_circ',\n",
    "    'calf_circ': 'panturrilha_circ',\n",
    "    'abd_circ': 'abdomen_circ',\n",
    "    'biceps_circ': 'biceps_circ'\n",
    "}, inplace=True)\n",
    "\n",
    "df_medidas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210be2e-9101-4d95-b39e-44ef96c4dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## comparando o total de linhas no arquivo CSV com o total de imagens\n",
    "\n",
    "if len(os.listdir(destination_path)) * 2 != df_medidas.shape[0]:\n",
    "   print(\"numero de linha não é igual ao total de imagens extraídas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d7c07-161e-4144-93ce-c5d7c535081e",
   "metadata": {},
   "source": [
    "Conforme observado na célula anterior, algumas imagens reportadas no CSV não estão contidas no pasta que extraimos com aquivos .pngs da próprias imagens. Então, vamos filtrar o Dataframe 'df_medidas' para manter apenas os IDs presentes nos arquivos de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae4b39-75a0-4978-ab26-4321a684f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtrando df_medidas para manter apenas os IDs presentes nos arquivos de imagens\n",
    "\n",
    "# Obter ids das imagens\n",
    "ids_normes_arquivos = os.listdir(destination_path)\n",
    "# Obter somente medidas de amostras que estão associadas com imagens\n",
    "df_medidas_filtrado = df_medidas[df_medidas['id'].isin(ids_normes_arquivos)].reset_index(drop=True)\n",
    "\n",
    "print(\"Antes: \", df_medidas.shape[0])\n",
    "print(\"Depois: \", df_medidas_filtrado.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642d224-7e2e-4499-bf7a-bd861ad56a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medidas_filtrado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fd412-c57c-4894-8bea-11eb0ae283dc",
   "metadata": {},
   "source": [
    "## Divisão em treino, validação e teste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f76ce-ccaf-4b22-8250-7147a93d454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verificando contagem de amostras por tipo de split (train, validation, test, unknown)\n",
    "\n",
    "df_medidas_filtrado['split'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f709a-4ea2-4652-afa1-4b28cf068f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agora vou particionar o Dataframe por tipo de split. Note que unknown será considerado teste.\n",
    "\n",
    "df_medidas_filtrado_treino = df_medidas_filtrado[df_medidas_filtrado['split'] == 'train'].reset_index(drop=True)\n",
    "treino_ids = df_medidas_filtrado_treino.id.tolist()\n",
    "\n",
    "df_medidas_filtrado_val = df_medidas_filtrado[df_medidas_filtrado['split'] == 'val'].reset_index(drop=True)\n",
    "val_ids = df_medidas_filtrado_val.id.tolist()\n",
    "\n",
    "\n",
    "df_medidas_filtrado_teste = df_medidas_filtrado[df_medidas_filtrado['split'].isin(['test', 'unknown'])].reset_index(drop=True)\n",
    "teste_ids = df_medidas_filtrado_teste.id.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5c7d2-6506-4f62-a378-2b9312801d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medidas_filtrado_treino.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efc7dd-cbd7-4c25-9c7c-5d752b73ca47",
   "metadata": {},
   "source": [
    "Como o dataframe originado do arquivo CSV somente possui informações associadas com um ID único para cada amostra e sabendo que cada amostra possui duas imagens para a parte frontal e lateral do corpo, então vou obter o caminho completo de cada imagem associada com o ID por tipo de split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706869e2-7e1a-4b25-9eef-6f952ab6200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorios_imagens = list_image_files(destination_path)\n",
    "\n",
    "## Filtrando os caminhos cujas subpastas estão em train_ids\n",
    "treino_diretorios_imagens = [\n",
    "    path for path in diretorios_imagens\n",
    "    if os.path.basename(os.path.dirname(path)) in treino_ids\n",
    "]\n",
    "## Filtrando os caminhos cujas subpastas estão em val_ids\n",
    "validacao_diretorios_imagens = [\n",
    "    path for path in diretorios_imagens\n",
    "    if os.path.basename(os.path.dirname(path)) in val_ids\n",
    "]\n",
    "\n",
    "## Filtrando os caminhos cujas subpastas estão em teste_ids\n",
    "teste_diretorios_imagens = [\n",
    "    path for path in diretorios_imagens\n",
    "    if os.path.basename(os.path.dirname(path)) in teste_ids\n",
    "]\n",
    "\n",
    "print(\"Total de imagens para treino: \", len(treino_diretorios_imagens))\n",
    "print(\"Total de imagens para validação: \", len(validacao_diretorios_imagens))\n",
    "print(\"Total de imagens para teste: \", len(teste_diretorios_imagens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4c3f6-5ec7-422e-8eb1-e22dd64117de",
   "metadata": {},
   "source": [
    "Observe que o toal de imagens por split é justamente o dobro de amostras no dataframe associado com aquele split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afd6e4-2ef9-49ba-8667-bf3c264ccdac",
   "metadata": {},
   "source": [
    "Agora que possuimos os caminho completos das imagens e informações associadas com imagem, então podemos prosseguir com a criação do Dataset Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e6319-b1c3-4983-b379-b47140cd0a9a",
   "metadata": {},
   "source": [
    "## Criando Dataset Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2e2e4-3ed9-4de5-a172-c43207cc7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criação de um scaler para efetura Z-Score Normalization para cada split. Essa normalização vai\n",
    "# ser útil para padronizar os vetores de características multivaridos que devemos estimar por\n",
    "# regressão. É uma estretégia conhecida de Séries Temporaris. \n",
    "# A motivação é que Funções de Perda comuns, como MSE (Erro Quadrático Médio), são sensíveis à magnitude. Valores maiores dominam a perda, \n",
    "# mesmo que sejam menos significativos.\n",
    "\n",
    "\n",
    "criar_scaler_para_split(\n",
    "    dataframe=df_medidas_filtrado_treino,\n",
    "    dir_arquivo_saida=\"treino_scaler.pkl\"\n",
    ")\n",
    "\n",
    "\n",
    "criar_scaler_para_split(\n",
    "    dataframe=df_medidas_filtrado_val,\n",
    "    dir_arquivo_saida=\"validacao_scaler.pkl\"\n",
    ")\n",
    "\n",
    "\n",
    "criar_scaler_para_split(\n",
    "    dataframe=df_medidas_filtrado_teste,\n",
    "    dir_arquivo_saida=\"teste_scaler.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c729ba1-b5f7-44f0-8270-6a6c59b172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class U2NetToTensor(object):\n",
    "    \"\"\"\n",
    "    Versão levemente modificada de\n",
    "    https://github.com/xuebinqin/U-2-Net/blob/ac7e1c817ecab7c7dff5ce6b1abba61cd213ff29/data_loader.py#L103\n",
    "\n",
    "    Converte uma imagem ndarray para tensor.\n",
    "    Esta versão ignora 'label' e 'imidx', retornando apenas a imagem normalizada como tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Normaliza a imagem para [0, 1]\n",
    "        image = image / np.max(image)\n",
    "\n",
    "        # Inicializa imagem com 3 canais\n",
    "        tmpImg = np.zeros((image.shape[0], image.shape[1], 3))\n",
    "\n",
    "        # Aplica normalização tipo ImageNet\n",
    "        if image.shape[2] == 1:\n",
    "            tmpImg[:, :, 0] = (image[:, :, 0] - 0.485) / 0.229\n",
    "            tmpImg[:, :, 1] = (image[:, :, 0] - 0.485) / 0.229\n",
    "            tmpImg[:, :, 2] = (image[:, :, 0] - 0.485) / 0.229\n",
    "        else:\n",
    "            tmpImg[:, :, 0] = (image[:, :, 0] - 0.485) / 0.229\n",
    "            tmpImg[:, :, 1] = (image[:, :, 1] - 0.456) / 0.224\n",
    "            tmpImg[:, :, 2] = (image[:, :, 2] - 0.406) / 0.225\n",
    "\n",
    "        # Transforma para formato [C, H, W]\n",
    "        tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "\n",
    "        # Retorna apenas o tensor da imagem\n",
    "        return torch.from_numpy(tmpImg).float()\n",
    "\n",
    "\n",
    "class RegressaoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para nossa tarefa de regressão\n",
    "\n",
    "    Este dataset associa imagens a vetores de rótulos numéricos extraídos dos DataFrames dos splits originado do aqruivo de medidas.\n",
    "    Os rótulos podem ser opcionalmente normalizados com um objeto scaler (ex: StandardScaler).\n",
    "\n",
    "    As imagens são transformadas com redimensionamento, conversão para tensor e normalizadas no intervalo 0,1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths : list of str\n",
    "        Lista de caminhos absolutos para os arquivos de imagem.\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame contendo os rótulos associados a cada imagem. Deve conter uma coluna 'id'\n",
    "        que corresponde ao nome da pasta onde a imagem está localizada.\n",
    "    scaler : object\n",
    "        Objeto de normalização (ex: StandardScaler) com método `.transform()` para aplicar nos rótulos.\n",
    "    aplicar_rotulo_scaler : bool, optional\n",
    "        Se True, aplica o scaler aos rótulos. Caso contrário, os rótulos são mantidos em seu formato original.\n",
    "        Default é False.\n",
    "    exclude_cols : list of str, optional\n",
    "        Lista de colunas a serem excluídas do conjunto de rótulos. Default é ['id', 'height', 'split'].\n",
    "    u2net : bool, optional\n",
    "        Se True, aplica o preprocessamento da U2Net. Caso contrário, o prepocessamento com ToTensor é aplicado para normalizar a imagem converter \n",
    "        de PIL para Tensor com valores intervalo [0,1]. [Atenção]: esse argumento deve ser True se você está usando U2Net durante o treinamento\n",
    "        ou inferência.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, dataframe, scaler, aplicar_rotulo_scaler=False, exclude_cols=['id', 'height', 'split'], u2net=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.df = dataframe\n",
    "\n",
    "        if unet != True\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((512, 512)),\n",
    "                T.ToTensor(), # normaliza [0,1]\n",
    "               # T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "               #             std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((512, 512)),\n",
    "                U2NetToTensor(), # normaliza [0,1]\n",
    "            ])\n",
    "            \n",
    "    \n",
    "        self.label_cols = [col for col in self.df.columns if col not in exclude_cols]\n",
    "\n",
    "        if aplicar_rotulo_scaler:\n",
    "            self.id_to_label = {\n",
    "                row['id']: scaler.transform(row[self.label_cols].values.astype('float32').reshape(1, -1))\n",
    "                for _, row in self.df.iterrows()\n",
    "            }\n",
    "        else:\n",
    "            self.id_to_label = {\n",
    "                row['id']: row[self.label_cols].values.astype('float32').reshape(1, -1)\n",
    "                for _, row in self.df.iterrows()\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retorna o número total de amostras no dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Número de imagens no dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Recupera a imagem e o vetor de rótulos correspondente ao índice fornecido.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Índice da amostra desejada.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor\n",
    "            Imagem transformada como tensor normalizado.\n",
    "        label : torch.Tensor\n",
    "            Vetor de rótulos associado à imagem, convertido para tensor float32.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Se o ID da imagem não for encontrado no mapeamento de rótulos.\n",
    "        \"\"\"\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        folder_id = os.path.basename(os.path.dirname(img_path))\n",
    "        label = self.id_to_label.get(folder_id)\n",
    "\n",
    "        if label is None:\n",
    "            raise ValueError(f\"ID '{folder_id}' não encontrado no split.\")\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d75bc7-550b-4063-97fd-4d3f4dd57f5f",
   "metadata": {},
   "source": [
    "Observe abaixo que apliquei o scaler somente no treino, pois a validação é uma simulação do teste. O scaler possui  método 'inverse_transform' responsável por transformar as valores padronizados para o espaço dos valores reais das medidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111862b4-efc1-495a-9a64-203bd542e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "treino_scaler = joblib.load(\"/content/treino_scaler.pkl\")\n",
    "val_scaler = joblib.load(\"/content/validacao_scaler.pkl\")\n",
    "teste_scaler = joblib.load(\"/content/teste_scaler.pkl\")\n",
    "\n",
    "treino_dataset = RegressaoDataset(\n",
    "    image_paths = treino_diretorios_imagens,\n",
    "    scaler = treino_scaler,\n",
    "    aplicar_rotulo_scaler=True,\n",
    "    dataframe = df_medidas_filtrado_treino,\n",
    "    u2net=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = RegressaoDataset(\n",
    "    image_paths = validacao_diretorios_imagens,\n",
    "    scaler = val_scaler,\n",
    "    aplicar_rotulo_scaler=False,\n",
    "    dataframe = df_medidas_filtrado_val,\n",
    "    u2net=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "teste_dataset = RegressaoDataset(\n",
    "    image_paths = teste_diretorios_imagens,\n",
    "    scaler = teste_scaler,\n",
    "    aplicar_rotulo_scaler=False,\n",
    "    dataframe = df_medidas_filtrado_teste,\n",
    "    u2net=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2965b-5a3e-4ec8-906c-004f2e987006",
   "metadata": {},
   "source": [
    "# Modelagem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad7be6c4-4e1d-4917-b2e3-976139a5cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModule:\n",
    "    \"\"\"\n",
    "    Segmentação semântica da pessoa na imagem.\n",
    "    - Usa modelo DeepLabV3 pré-treinado do torchvision.\n",
    "    - Retorna a máscara binária da pessoa (silhueta) e a imagem original.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Carrega modelo DeepLabV3 com backbone ResNet50, pré-treinado no COCO\n",
    "        self.model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "        self.model.eval()  # modo avaliação, desativa dropout etc.\n",
    "\n",
    "        # Pré-processamento: redimensionar para 512x512 e converter para tensor\n",
    "        self.preprocess = transforms.Compose([\n",
    "            T.Resize((512, 512)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def extract_silhouette(self, image_path):\n",
    "        # Abrir imagem com PIL e converter para RGB\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Pré-processar e adicionar dimensão batch (1, C, H, W)\n",
    "        input_tensor = self.preprocess(img).unsqueeze(0)\n",
    "\n",
    "        # Forward pass do modelo para segmentação\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)['out'][0]  # saída do modelo\n",
    "\n",
    "        # COCO class 15 = pessoa\n",
    "        mask = (output.argmax(0) == 15).byte().cpu().numpy()  # 1=pessoa, 0=fundo\n",
    "        mask_img = mask * 255  # escala para 0-255 para visualização\n",
    "\n",
    "        # Mostrar imagem original e silhueta lado a lado\n",
    "        fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].set_title(\"Imagem Original\")\n",
    "        axs[1].imshow(mask_img, cmap=\"gray\")\n",
    "        axs[1].set_title(\"Silhueta Extraída\")\n",
    "        for ax in axs: ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Retorna máscara binária e imagem original como array numpy\n",
    "        return mask_img, np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9dde2-70fb-4945-af9a-abe66326395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabSegmentationModule:\n",
    "    \"\"\"\n",
    "    Segmentação semântica da pessoa na imagem.\n",
    "    - Usa modelo DeepLabV3 pré-treinado do torchvision.\n",
    "    - Retorna a máscara binária da pessoa (silhueta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        # Carrega modelo DeepLabV3 com backbone ResNet50, pré-treinado no COCO\n",
    "        self.model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True).to(device)\n",
    "        self.model = self.model.eval()  # modo avaliação, desativa dropout etc.\n",
    "\n",
    "    def visualizar_silhueta(self, mask_img):\n",
    "        # Mostrar imagem original e silhueta lado a lado\n",
    "        mask_img = mask * 255  # escala para 0-255 para visualização\n",
    "        fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "        axs[0].imshow(img.cpu().squeeze().permute((1,2,0)).detach().numpy())\n",
    "        axs[0].set_title(\"Imagem Original\")\n",
    "        axs[1].imshow(mask_img, cmap=\"gray\")\n",
    "        axs[1].set_title(\"Silhueta Extraída\")\n",
    "        for ax in axs: ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def extract_silhouette(self, input_tensor):\n",
    "        # Forward pass do modelo para segmentação\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)['out'][0]  # saída do modelo\n",
    "\n",
    "        # COCO class 15 = pessoa\n",
    "        mask = (output.argmax(0) == 15).byte().detach().cpu().numpy()  # 1=pessoa, 0=fundo\n",
    "\n",
    "        # Retorna máscara binária \n",
    "        return mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1fccace-cb52-4526-8fc1-daa230fe2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsModule:\n",
    "    \"\"\"\n",
    "    Detecta pontos-chave aproximados da silhueta da pessoa.\n",
    "    Abordagem simplificada sem usar MediaPipe:\n",
    "    - Usa o contorno da silhueta para localizar o corpo.\n",
    "    - Calcula uma bounding box (retângulo) ao redor do contorno.\n",
    "    - Estima keypoints com base em proporções dentro da bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_keypoints(self, mask):\n",
    "        # Encontrar todos os contornos na máscara\n",
    "        # cv2.RETR_EXTERNAL -> pega apenas os contornos externos\n",
    "        # cv2.CHAIN_APPROX_SIMPLE -> reduz número de pontos do contorno\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            # Nenhum contorno encontrado → retorna dicionário vazio\n",
    "            return {}\n",
    "\n",
    "        # Seleciona o maior contorno, que corresponde à pessoa\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Calcula a bounding box do contorno\n",
    "        # x, y -> canto superior esquerdo da caixa\n",
    "        # w, h -> largura e altura da caixa\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Estima keypoints com base em proporções da bounding box\n",
    "        keypoints = {\n",
    "            'top_head': (x + w//2, y),                # topo da cabeça\n",
    "            'neck': (x + w//2, y + h//10),           # pescoço (10% da altura)\n",
    "            'shoulders': (x + w//2, y + h//5),       # ombros (20% da altura)\n",
    "            'waist': (x + w//2, y + h//2),           # cintura (50% da altura)\n",
    "            'hips': (x + w//2, y + int(h*0.6)),      # quadril (60% da altura)\n",
    "            'knees': (x + w//2, y + int(h*0.8)),     # joelhos (80% da altura)\n",
    "            'ankles': (x + w//2, y + h),             # tornozelos (base da bounding box)\n",
    "        }\n",
    "\n",
    "        # Retorna dicionário com keypoints aproximados\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba6468ce-66d7-4e93-8620-cffc7b9ba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegressionModule:\n",
    "    \"\"\"\n",
    "    Converte keypoints em medidas corporais aproximadas.\n",
    "    Para protótipo, gera valores simulados baseados em proporções.\n",
    "    \"\"\"\n",
    "    def predict_measures(self, keypoints):\n",
    "        measures = {}\n",
    "        if not keypoints:\n",
    "            return measures\n",
    "\n",
    "        # Exemplo: usar distância entre pontos como proxy\n",
    "        def distance(p1, p2):\n",
    "            return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "        measures['pescoço'] = distance(keypoints['top_head'], keypoints['neck']) * 1.2\n",
    "        measures['peito'] = distance(keypoints['shoulders'], keypoints['waist']) * 2\n",
    "        measures['cintura'] = distance(keypoints['waist'], keypoints['hips']) * 1.5\n",
    "        measures['quadril'] = distance(keypoints['hips'], keypoints['knees']) * 1.3\n",
    "        measures['coxa'] = distance(keypoints['hips'], keypoints['knees'])\n",
    "        measures['joelho'] = distance(keypoints['knees'], keypoints['ankles'])\n",
    "        measures['panturrilha'] = distance(keypoints['knees'], keypoints['ankles']) * 0.7\n",
    "        measures['abdomen'] = measures['cintura'] * 0.9\n",
    "        measures['biceps'] = measures['peito'] * 0.3\n",
    "\n",
    "        return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622ea9d5-0846-4bf7-be6e-86a92ee181dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medidas estimadas (aproximadas):\n",
      "pescoço: 43.20 px\n",
      "peito: 222.00 px\n",
      "cintura: 54.00 px\n",
      "quadril: 96.20 px\n",
      "coxa: 74.00 px\n",
      "joelho: 74.00 px\n",
      "panturrilha: 51.80 px\n",
      "abdomen: 48.60 px\n",
      "biceps: 66.60 px\n"
     ]
    }
   ],
   "source": [
    "# main_pipeline.py\n",
    "seg_module = SegmentationModule()\n",
    "kp_module = KeypointsModule()\n",
    "reg_module = RegressionModule()\n",
    "\n",
    "mask, image = seg_module.extract_silhouette(\"female-001289_B00.png\")\n",
    "keypoints = kp_module.extract_keypoints(mask)\n",
    "measures = reg_module.predict_measures(keypoints)\n",
    "\n",
    "print(\"Medidas estimadas (aproximadas):\")\n",
    "for k,v in measures.items():\n",
    "    print(f\"{k}: {v:.2f} px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62455a-66a1-4ee4-8bca-f32254a136de",
   "metadata": {},
   "source": [
    "## Modelagem U2Net\n",
    "\n",
    "\n",
    "Nesta seção, mostramos a implementação do código de inferência da U2Net.\n",
    "\n",
    "O código da arquitetura da U2Net foi copiado da implementação original dos autores, disponível em: [https://github.com/xuebinqin/U-2-Net](https://github.com/xuebinqin/U-2-Net). Utilizamos essa arquitetura para carregar o modelo de segmentação de pessoas, cujo *checkpoint* pode ser baixado neste link: [https://drive.google.com/file/d/1m_Kgs91b21gayc2XLW0ou8yugAIadWVP/view?usp=sharing](https://drive.google.com/file/d/1m_Kgs91b21gayc2XLW0ou8yugAIadWVP/view?usp=sharing). Esse *checkpoint* é fornecido no repositório oficial da U2Net no GitHub.\n",
    "\n",
    "- Criamos um módulo PyTorch chamado `U2NetSegmentationModule`, que carrega os parâmetros do arquivo `u2net_human_seg.pth` na arquitetura da U2Net. Para facilitar a abstração do sistema, o `U2NetSegmentationModule` foi desenvolvido com uma interface de métodos compatível com os módulos `SegmentationModule` e `DeepLabSegmentationModule`, permitindo redução de alterações majoritárias no código de inferência.\n",
    "\n",
    "- Note que, ao criar o `RegressaoDataset`, é necessário definir o argumento `u2net` do construtor da classe como `True` antes de realizar inferência ou treinamento com esse modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0cbee-ef67-4220-a8a0-12553467aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title U2Net\n",
    "\n",
    "# código copiado de https://github.com/xuebinqin/U-2-Net/blob/master/model/u2net.py\n",
    "class REBNCONV(nn.Module):\n",
    "    def __init__(self,in_ch=3,out_ch=3,dirate=1):\n",
    "        super(REBNCONV,self).__init__()\n",
    "\n",
    "        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n",
    "        self.bn_s1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu_s1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))\n",
    "\n",
    "        return xout\n",
    "\n",
    "## upsample tensor 'src' to have the same spatial size with tensor 'tar'\n",
    "def _upsample_like(src,tar):\n",
    "\n",
    "    src = F.interpolate(src,size=tar.shape[2:],mode='bilinear') # to avoid depreciated\n",
    "    #F.upsample(src,size=tar.shape[2:],mode='bilinear')\n",
    "\n",
    "    return src\n",
    "\n",
    "\n",
    "### RSU-7 ###\n",
    "class RSU7(nn.Module):#UNet07DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU7,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "        hx = self.pool5(hx5)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx)\n",
    "\n",
    "        hx7 = self.rebnconv7(hx6)\n",
    "\n",
    "        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),1))\n",
    "        hx6dup = _upsample_like(hx6d,hx5)\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-6 ###\n",
    "class RSU6(nn.Module):#UNet06DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU6,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx5)\n",
    "\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-5 ###\n",
    "class RSU5(nn.Module):#UNet05DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU5,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4 ###\n",
    "class RSU4(nn.Module):#UNet04DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4F ###\n",
    "class RSU4F(nn.Module):#UNet04FRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4F,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx2 = self.rebnconv2(hx1)\n",
    "        hx3 = self.rebnconv3(hx2)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3d,hx2),1))\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2d,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "\n",
    "##### U^2-Net ####\n",
    "class U2NET(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch=3,out_ch=1):\n",
    "        super(U2NET,self).__init__()\n",
    "\n",
    "        self.stage1 = RSU7(in_ch,32,64)\n",
    "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(64,32,128)\n",
    "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(128,64,256)\n",
    "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(256,128,512)\n",
    "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(512,256,512)\n",
    "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(512,256,512)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(1024,256,512)\n",
    "        self.stage4d = RSU4(1024,128,256)\n",
    "        self.stage3d = RSU5(512,64,128)\n",
    "        self.stage2d = RSU6(256,32,64)\n",
    "        self.stage1d = RSU7(128,16,64)\n",
    "\n",
    "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n",
    "        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n",
    "        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "\n",
    "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        #stage 1\n",
    "        hx1 = self.stage1(hx)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        #stage 2\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        #stage 3\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        #stage 4\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        #stage 5\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        #stage 6\n",
    "        hx6 = self.stage6(hx)\n",
    "        hx6up = _upsample_like(hx6,hx5)\n",
    "\n",
    "        #-------------------- decoder --------------------\n",
    "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "\n",
    "        #side output\n",
    "        d1 = self.side1(hx1d)\n",
    "\n",
    "        d2 = self.side2(hx2d)\n",
    "        d2 = _upsample_like(d2,d1)\n",
    "\n",
    "        d3 = self.side3(hx3d)\n",
    "        d3 = _upsample_like(d3,d1)\n",
    "\n",
    "        d4 = self.side4(hx4d)\n",
    "        d4 = _upsample_like(d4,d1)\n",
    "\n",
    "        d5 = self.side5(hx5d)\n",
    "        d5 = _upsample_like(d5,d1)\n",
    "\n",
    "        d6 = self.side6(hx6)\n",
    "        d6 = _upsample_like(d6,d1)\n",
    "\n",
    "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
    "\n",
    "        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74138a-6e9f-4cf1-b54b-e333ea223acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U2NetSegmentationModule:\n",
    "    \"\"\"\n",
    "        Detecção de Objeto Saliente pessoa na imagem.\n",
    "        - Usa modelo U2Net pré-treinado para segmentar pessoas.\n",
    "        - Retorna a máscara binária da pessoa (silhueta) após limiarização da saída da U2Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir):\n",
    "        \"\"\"\n",
    "        model_dir: diretório do modelo U2Net\n",
    "        \"\"\"\n",
    "        # Carrega modelo U2Net pretreinada\n",
    "        self.model = U2NET(3,1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          self.model.load_state_dict(torch.load(model_dir))\n",
    "          self.model.to(\"cuda\")\n",
    "        else:\n",
    "          self.model.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
    "\n",
    "        self.model = self.model.eval()  # modo avaliação\n",
    "\n",
    "    def normPRED(d):\n",
    "      # função copiada de https://github.com/xuebinqin/U-2-Net/tree/master\n",
    "      \"\"\"\n",
    "      Realiza normalização MIN-MAX na entrada.\n",
    "      Args:\n",
    "        - d: Mapa de Saliência de entrada. Numpy array de tamanho HxWxC\n",
    "      \"\"\"\n",
    "      ma = torch.max(d)\n",
    "      mi = torch.min(d)\n",
    "\n",
    "      dn = (d-mi)/(ma-mi)\n",
    "\n",
    "      return dn\n",
    "\n",
    "    def otsu_binarization(inp):\n",
    "      \"\"\"\n",
    "        Realiza a binarização adaptativa de OTSU em uma imagem de entrada\n",
    "\n",
    "        Args:\n",
    "          - inp: Mapa de Saliência de entrada. Numpy array de tamanho HxWxC\n",
    "\n",
    "        Return:\n",
    "          - mask: Máscara binária. Numpy array of shape HxWx1\n",
    "      \"\"\"\n",
    "\n",
    "      # limiarização adaptativa com OTSU\n",
    "      otsu_thresh = threshold_otsu(inp)\n",
    "      mask = (inp > otsu_thresh).astype(np.uint8) # 1: objeto, 0: fundo\n",
    "      return mask\n",
    "\n",
    "\n",
    "\n",
    "    def visualizar_silhueta(self, img, mask):\n",
    "        # Mostrar imagem original e silhueta lado a lado\n",
    "        mask_img = mask * 255  # escala para 0-255 para visualização\n",
    "        fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "        axs[0].imshow(img.cpu().squeeze().permute((1,2,0)).detach().numpy())\n",
    "        axs[0].set_title(\"Imagem Original\")\n",
    "        axs[1].imshow(mask_img, cmap=\"gray\")\n",
    "        axs[1].set_title(\"Silhueta Extraída\")\n",
    "        for ax in axs: ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def extract_silhouette(self, input_tensor):\n",
    "        # Forward pass do modelo para segmentação\n",
    "        with torch.no_grad():\n",
    "             saliencia,_,_,_,_,_,_ =  self.model(input_tensor)  # saída do modelo\n",
    "\n",
    "        # convertendo saliencia do intervalo [0,1] para máscara binária\n",
    "        pred = saliencia[:,0,:,:]\n",
    "        pred = U2NetSegmentationModule.normPRED(pred)\n",
    "\n",
    "        pred = pred.squeeze()\n",
    "        pred = pred.cpu().data.numpy()\n",
    "\n",
    "        # limiarização adaptativa com OTSU\n",
    "        mask = U2NetSegmentationModule.otsu_binarization(pred) # 1: objeto, 0: fundo\n",
    "\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fca0b-345c-4c3f-a172-5186d75da323",
   "metadata": {},
   "source": [
    "## Modelagem e Treinamento do XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3a791-61c3-49de-ba00-ad79fd2f3c4f",
   "metadata": {},
   "source": [
    "### Preparação dos dados para treinar o XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e16c77-07b6-4c7f-8e5f-7c20ec04174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Criação do DataLoader\n",
    "treino_loader = DataLoader(\n",
    "    dataset=treino_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #pin_memory=True,\n",
    "    #num_workers=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5488456-cba3-4a83-8849-1d1fb1bc2e9a",
   "metadata": {},
   "source": [
    "Considerando que devemos extrair informações de pontos chaves, então criamos o módulo reponsável por extrair\n",
    "um vetor de características onde cada componente deste vetor corresponde à norma euclidiana (distância) entre N corrdenadas 2D de pontos chaves detectados da silhueta, porém devido a simetria da distância euclidiana e para evitar redundância de valores, usamos apenas pares de pontos únicos sem repetição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde93d4-4825-46ae-962b-cb645cd69542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExtractFeaturesKeypoints(nn.Module):\n",
    "    \"\"\"\n",
    "    Extrai um vetor de características com as distâncias euclidianas entre pares únicos de keypoints.\n",
    "\n",
    "    Para N keypoints, o vetor de saída terá N(N-1)/2 valores, correspondentes às distâncias entre\n",
    "    todos os pares distintos (sem repetição e sem pares consigo mesmo).\n",
    "\n",
    "    Parâmetros:\n",
    "        keypoints (dict): Dicionário com keypoints nomeados. Exemplo:\n",
    "            {\n",
    "                'top_head': (x1, y1),\n",
    "                'neck': (x2, y2),\n",
    "                ...\n",
    "            }\n",
    "\n",
    "    Retorno:\n",
    "        torch.Tensor: Vetor 1D com shape (N(N-1)/2), contendo as distâncias euclidianas entre pares únicos.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ExtractFeaturesKeypointsV2, self).__init__()\n",
    "\n",
    "    def forward(self, keypoints: dict):\n",
    "        coords = torch.tensor(list(keypoints.values()), dtype=torch.float32)  # shape: [N, 2]\n",
    "        pairs = list(combinations(range(len(coords)), 2))  # isso gera pares únicos de indices da lista coords (keypoint)\n",
    "\n",
    "        distances = []\n",
    "        for i, j in pairs:\n",
    "            dist = torch.norm(coords[i] - coords[j])\n",
    "            distances.append(dist)\n",
    "\n",
    "        return torch.stack(distances)  # total: [N(N-1)/2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13e5b7-45a1-4565-8ca7-15c9889cf3a3",
   "metadata": {},
   "source": [
    "A função `generate_regressor_data` a seguir vai gerar e salvar os tensores que armazenam as features extraídas do pontos chaves\n",
    "e os vetores de medidas alvos que devemos estimar. Em suma, armazenamos um tensor de vetores de caractéricas extraídas dos pontos chanve com tamanho `Bx21` onde B é o toal de amostras e 21 é o toal de distâncias computadas no módulo `ExtractFeaturesKeypoints`, enquanto outro tensor possui o tamanho `Bx9` para as nove medidas corporais que devemos regredir. Um modelo de Regressão Múltipla de Aprendizado de Máquina ou Aprendizado Profundo deve aprender uma função não linear que vai converter o vetor  `Bx21` para o vetor `Bx9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8daf4a-11bf-46ff-b720-79f8a1458818",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_regressor_data(save_tensors=False):\n",
    "  features_list = []\n",
    "  labels_list = []\n",
    "  # 30 minutos\n",
    "  for img, label in tqdm(treino_loader, desc=\"Criando dados de treino da MLP\"):\n",
    "    img, label = img.to(device), label.to(device) # move a imagem e os rótulos para o dispositivo.\n",
    "\n",
    "    mask = seg_module.extract_silhouette(img) # extrai a silhueta da imagem.\n",
    "\n",
    "    # se a silhueta estiver vazia, usa medidas zeradas como fallback\n",
    "    if mask.max() == 0:\n",
    "          print(\"Máscara vazia\")\n",
    "          features = torch.zeros(100).to(device)  # fallback\n",
    "    else:\n",
    "          keypoints = kp_module.extract_keypoints(mask)\n",
    "          features = feature_extractor(keypoints).to(device)\n",
    "  \n",
    "    label = label.view(-1).to(device)\n",
    "\n",
    "    features_list.append(features.cpu())\n",
    "    labels_list.append(label.cpu())\n",
    "\n",
    "     \n",
    "  # Conversão para tensores e salva\n",
    "  X_treino_regressao = torch.stack(features_list)  # shape: [total_amostras, total_distancias]\n",
    "  y_treino_regressao = torch.stack(labels_list)    # shape: [total_amostras, total_medidas_alvo]\n",
    "\n",
    "  if save_tensors:\n",
    "    torch.save(X_treino_regressao, \"X_treino_regressao.pt\")\n",
    "    torch.save(y_treino_regressao, \"y_treino_regressao.pt\")\n",
    "   \n",
    "  return X_treino_regressao, y_treino_regressao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b6bc3-21be-41e9-860b-58489e680dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seg_module = U2NetSegmentationModule(\"/content/u2net_human_seg.pth\") #U2NetSegmentationModule(\"/content/u2net_human_seg.pth\")#SAMQuantizedSegmentationModule(model_path=\"/content/sam_onnx_quantized_example.onnx\", device=device)#SAMSegmentationModule(device=device)#DeepLabSegmentationModule(device=device)\n",
    "kp_module = KeypointsModule()\n",
    "feature_extractor = ExtractFeaturesKeypointsV2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de5edd-0f0e-42e1-95ee-9416521e811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino_regressao, y_treino_regressao = generate_regressor_data(save_tensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bf32f-b202-4737-a833-c95f3a8bd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino_regressao_numpy = X_treino_regressao.cpu().detach().numpy()\n",
    "y_treino_regressao_numpy =  y_treino_regressao.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a8925-8c94-4e71-8b26-14aadc9c013c",
   "metadata": {},
   "source": [
    "Antes de iniciar o treinamento do XGBoost, vamos preparar os vetores de caracteríticas de entrada usando uma normalização similar aquela normalização usada nas medidas alvos no `RegressaoDataset`. Nesse, vamos apenas dividir cada vetor de características (distancias euclidianas computadas) extraídos dos pontos chave por seu valor máximo. Isso pode ser útil pois podemos os valores na mesma escala o que certamente facilita o aprendizado do modelo e evita que \"features\" com valores maiores dominem sobre outras. Em suma, essa normalização simples, baseada no valor máximo de cada vetor, deve garante que todas as distâncias fiquem em uma faixa comparável, geralmente entre 0 e 1. Além disso, essa normalização deve preservar as proporções relativas entre os pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47c698-a141-43c5-92ba-fcd65ffc0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximo_linha = np.max(X_treino_regressao_numpy, axis=1, keepdims=True) # computa o maior valor de cada vetor no conjunto de treino de entrada\n",
    "print(maximo_linha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa284483-1cd9-448a-b820-d646024c113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino_regressao_numpy = X_treino_regressao_numpy / maximo_linha + 1e-10 # normalizando entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed958a-7564-474c-bc3e-8cf1f0b09674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Criando uma instância do XGBRegressor\n",
    "base_model = XGBRegressor(n_estimators=100, learning_rate=0.1, device='gpu', random_state=2025)\n",
    "\n",
    "# Wrap o XGBoost com MultiOutputRegressor\n",
    "model = MultiOutputRegressor(base_model)\n",
    "start = time.time()\n",
    "# Treinando o modelo\n",
    "model.fit(X_treino_regressao_numpy, y_treino_regressao_numpy)\n",
    "print(\"XGBoost tempo de treinamento:\", time.time() - start)\n",
    "\n",
    "# Salvando o modelo treinado \n",
    "joblib.dump(model, 'multioutput_xgb_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758becca-4042-4383-a54f-0e0a4b881964",
   "metadata": {},
   "source": [
    "## Modelagem e Treinamento do Regressor baseado em DepthwiseSeparableConvolutioin 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75c70e-1935-44ca-8684-a133d7de7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um simple Dataset Pytorch para carregar os tensores em batchs\n",
    "class RegressorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e80f37-b41e-4ffa-a139-3ad4f9ac44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 120  # ou outro conforme a capacidade da GPU\n",
    "regressor_treino_dataset = RegressorDataset(X_treino_regressao, y_treino_regressao)\n",
    "regressor_treino_loader = DataLoader(regressor_treino_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dca86-fb1a-446d-972f-12030610033a",
   "metadata": {},
   "source": [
    "O módulo `DepthwiseSeparableConv1DRegressor` visa reduzir o número de parâmetros e operações — ideal para implantação em dispositivos móveis. Quando `groups=in_channels` faz com que o Conv1d opere independentemente em cada canal (em profundidade). Por outro lado, quando\n",
    "vkernel_size=1` usando apenas uma projeção não linear que agrega canais (em pontos no feature map). Usamos apenas Conv1D pois estamos transformação um sinal 1D (vetor de características) para outro sinal 1D (vetor de medidas corporais).\n",
    "\n",
    "\n",
    "O conceito de Depthwise Separable Convolutioin é usado é arquitetura famosdas como MobileNet e MobileVit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377eff7-db26-4f90-86c7-23de07890b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DepthwiseSeparableConv1DRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Esse módulo usa dois componentes pricipais: \n",
    "    a) Depthwise convolution: aplica um único filtro convolucional por canal de feature de entrada (sem agregação de informação)\n",
    "    b) Pointwise convolution: usa convolução 1×1 para agregar os canais.\n",
    "\n",
    "    \n",
    "    # código baseado de https://github.com/seungjunlee96/Depthwise-Separable-Convolution_Pytorch/tree/master\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1, input_length=21, output_size=9):\n",
    "        super(DepthwiseSeparableConv1DRegressor, self).__init__()\n",
    "        #https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        self.depthwise = nn.Conv1d(in_channels=input_channels, out_channels=input_channels,\n",
    "                                   kernel_size=3, padding=1, groups=input_channels)\n",
    "        self.pointwise = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1, groups=16),  # depthwise\n",
    "            nn.Conv1d(32, 32, kernel_size=1),  # pointwise\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * input_length, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pointwise(self.depthwise(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Configurações e instanticação do modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DepthwiseSeparableConv1DRegressor().to(device)\n",
    "criterion = nn.MSELoss()  # ou nn.CrossEntropyLoss() se for classificação\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2f93c-bc3c-41bc-94fb-0e56ea74d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "# Número de épocas\n",
    "num_epochs = 2000\n",
    "history = {\"train_loss\":[]}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Treinando. Epochs: \"):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in regressor_treino_loader:\n",
    "        features, labels = batch  # features: [B, 21], labels: [B, 9]\n",
    "\n",
    "        # Preprocessamento\n",
    "        features = features.view(features.size(0), 1, 21).to(device)\n",
    "        labels = labels.view(labels.size(0), -1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(treino_loader)\n",
    "    history['train_loss'].append(avg_loss)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "hours, rem = divmod(elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(f\"Tempo de execução de treinamento: {int(hours)}h {int(minutes)}min {int(seconds)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1bcac-eae3-4b95-95b0-a71ec3396e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando parâmetros do modelo\n",
    "torch.save(model.state_dict(), \"separable_conv_regressor_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09db5f-dbd3-4f2d-8501-1d3fca9e64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83e05a-22c7-40fa-bb48-34fd548c7178",
   "metadata": {},
   "source": [
    "Vamos aproveitar para criar um módulo similar ao RegressorModule. Observe que no módulo a seguir temos o método\n",
    "`predict_measures` que simplesmente realizar o mesmo preprocessamento da entrada que usamos no XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc91dc34-726e-4e89-a4e4-518814fe021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DepthwiseSeparableConv1DRegressorModule:\n",
    "    \"\"\"\n",
    "    Converte features extraídas dos keypoints em medidas corporais aproximadas usando DepthwiseSeparableConv1DRegressor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir, scaler_dir=None, device=None):\n",
    "        \"\"\"\n",
    "        model_dir: caminho para o modelo PyTorch (.pt ou .pth)\n",
    "        scaler_dir: caminho para o scaler sklearn (.pkl)\n",
    "        device: 'cuda' ou 'cpu'\n",
    "        \"\"\"\n",
    "        self.device = device \n",
    "\n",
    "        # Carrega modelo PyTorch\n",
    "        self.model = SeparableConv1DRegressor()#torch.load(model_dir, map_location=self.device)\n",
    "        self.model.load_state_dict(torch.load(model_dir, weights_only=True))\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Carrega scaler se fornecido\n",
    "        if scaler_dir is not None:\n",
    "            self.scaler_regressor = joblib.load(scaler_dir)\n",
    "        else:\n",
    "            self.scaler_regressor = None\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_feature(x):\n",
    "        \"\"\"\n",
    "        Aplica max-normalization para o vetor de característica de entrada.\n",
    "        \"\"\"\n",
    "        row_max = np.max(x, axis=1, keepdims=True)\n",
    "      \n",
    "        return x / row_max + 1e-10 # normalizando entrada\n",
    "\n",
    "    def predict_measures(self, keypoints_features):\n",
    "        \"\"\"\n",
    "        Recebe um vetor de features de pontos chance [1, 21] e retorna medidas corporais aproximadas em dicionário\n",
    "        \"\"\"\n",
    "        measures = {}\n",
    "        if keypoints_features is None:\n",
    "            return measures\n",
    "\n",
    "        # Normaliza\n",
    "        keypoints_features_norm = self.preprocess_feature(keypoints_features)\n",
    "\n",
    "        # Converte para tensor [1, 1, 21]\n",
    "        input_tensor = torch.tensor(keypoints_features_norm, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Predição\n",
    "        with torch.no_grad():\n",
    "            output_tensor = self.model(input_tensor)\n",
    "            measures_vector = output_tensor.cpu().detach().numpy()\n",
    "\n",
    "        # Inversão do scaler para converter predição para medidas aproximadas reais\n",
    "        if self.scaler_regressor is not None:\n",
    "            measures_vector = self.scaler_regressor.inverse_transform(measures_vector)\n",
    "\n",
    "        measures_vector = measures_vector.squeeze()  #  shape [1,9] -> shape [9]\n",
    "\n",
    "        measures['peito'] = measures_vector[0]\n",
    "        measures['cintura'] = measures_vector[1]\n",
    "        measures['quadril'] = measures_vector[2]\n",
    "        measures['coxa'] = measures_vector[3]\n",
    "        measures['joelho'] = measures_vector[4]\n",
    "        measures['panturrilha'] = measures_vector[5]\n",
    "        measures['abdomen'] = measures_vector[6]\n",
    "        measures['pescoço'] = measures_vector[7]\n",
    "        measures['biceps'] = measures_vector[8]\n",
    "\n",
    "        return measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037d3b2-457c-4483-a74d-092f19f9582d",
   "metadata": {},
   "source": [
    "# Avaliação \n",
    "\n",
    "\n",
    "Vamos usar a métrica de regressão Raiz do Erro Quadrático Médio, de Root Mean Squared Error (RMSE), que calcula\n",
    "a média da norma quadrática entre os valores previstos e valores verdadeiros. Dentro do problema de Regressão Multivariada, comparamos vetores previstos e verdadeira para cada amostra de validação ou amostra de teste de modo que o resultado final é a média entre o erro quadrático de todas as amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7a65c-ca90-43ba-bea8-272e9b93976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do DataLoader para o split de validacão\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #pin_memory=True,\n",
    "    #num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Criação do DataLoader para o split de teste\n",
    "test_loader = DataLoader(\n",
    "    dataset=teste_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #pin_memory=True,\n",
    "    #num_workers=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71781c02-5112-4fda-814d-81de9cefb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_average_rmse_gpu(loader, seg_module, kp_module, reg_module, feature_extractor_module=None, device, salvar_mascaras=False):\n",
    "    # O que é RMSE: https://c3.ai/glossary/data-science/root-mean-square-error-rmse/\n",
    "\n",
    "    \"\"\"\n",
    "    Calcula e imprime o erro quadrático médio (RMSE) entre os valores reais e os valores previstos\n",
    "    de medidas corporais em um problema de regressão multivariada, utilizando PyTorch.\n",
    "\n",
    "    A função percorre um DataLoader contendo imagens e seus respectivos rótulos (medidas reais),\n",
    "    extrai a silhueta da imagem, gera os keypoints, prediz as medidas com base nesses pontos,\n",
    "    e calcula o RMSE considerando o vetor de medidas como uma única predição multivariada por amostra.\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "    loader : torch.utils.data.DataLoader\n",
    "        DataLoader contendo tuplas (imagem, medidas reais).\n",
    "    \n",
    "    seg_module : módulo de segmentação\n",
    "        Módulo responsável por extrair a silhueta da imagem.\n",
    "    \n",
    "    kp_module : módulo de keypoints\n",
    "        Módulo responsável por extrair os pontos-chave da silhueta.\n",
    "    \n",
    "    reg_module : módulo de regressão\n",
    "        Módulo responsável por prever as medidas corporais a partir dos keypoints.\n",
    "        \n",
    "    feature_extractor_module: módulo de extração de carcaterísticas dos pontos-chave\n",
    "        Módulo responsável por extrair carcaterísticas dos pontos-chave da silhueta.\n",
    "        \n",
    "    device : torch.device\n",
    "        Dispositivo onde os tensores serão processados (CPU ou GPU).\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - Imprime o valor médio de RMSE em pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    total_erro_quadratico = 0.0\n",
    "    total_amostras = 0\n",
    "    count = 0 # apenas uma variável acumuladora para salvar máscaras em sequência\n",
    "    for img, label in tqdm(loader):\n",
    "        img, label = img.to(device), label.to(device) # move a imagem e os rótulos para o dispositivo.\n",
    "\n",
    "        mask = seg_module.extract_silhouette(img) # extrai a silhueta da imagem.\n",
    "        #seg_module.visualizar_silhueta(img, mask)\n",
    "        if salvar_mascaras:\n",
    "          salvar_silhuetas(img, mask, \"./resultado\", \"amostra_{}\".format(count))\n",
    "        \n",
    "        keypoints = kp_module.extract_keypoints(mask) # gera os keypoints da silhueta.\n",
    "\n",
    "        if feature_extractor_module is not None:\n",
    "             keypoints_features = feature_extractor_module(keypoints) # 1x21\n",
    "        \n",
    "        # prediz as medidas corporais com base nos keypoints.\n",
    "        if isinstance(reg_module, XGBoostRegressionModule):\n",
    "          measures = reg_module.predict_measures(np.expand_dims(keypoints_features.cpu().detach().numpy(), 0))\n",
    "        elif isinstance(reg_module, DepthwiseSeparableConv1DRegressorModule):\n",
    "          measures = reg_module.predict_measures(np.expand_dims(keypoints_features, 0))\n",
    "        else:\n",
    "          measures = reg_module.predict_measures(keypoints) \n",
    "            \n",
    "        # se a silhueta estiver vazia, usa medidas zeradas como fallback\n",
    "        if mask.max() == 0:\n",
    "            measures = {\n",
    "                'peito': 0.00, 'cintura': 0.00, 'quadril': 0.00, 'coxa': 0.00,\n",
    "                'joelho': 0.00, 'panturrilha': 0.00, 'abdomen': 0.00,\n",
    "                'pescoço': 0.00, 'biceps': 0.00\n",
    "            }\n",
    "\n",
    "        pred_tensor = torch.tensor(list(measures.values()), device=device)\n",
    "        pred_tensor = pred_tensor.unsqueeze(0) # 1x9\n",
    "        true_tensor = torch.tensor(label, device=device)\n",
    "        true_tensor = true_tensor.view(1, -1) # 1x9\n",
    "        assert pred_tensor.shape == true_tensor.shape\n",
    "\n",
    "        # calculando o erro quadrático da predição (norma ao quadrado da diferença)\n",
    "        erro_quadratico =  (pred_tensor - true_tensor).pow(2).sum().item()\n",
    "        # acumulando o erro total e o número de amostras.\n",
    "        total_erro_quadratico += erro_quadratico\n",
    "        total_amostras += 1 # N\n",
    "    \n",
    "    # Computando RMSE de MSE\n",
    "    erro_quadratico_medio = total_erro_quadratico / max(total_amostras, 1)\n",
    "    raiz_erro_quadratico_medio = math.sqrt(erro_quadratico_medio)\n",
    "    \n",
    "    print(f\"\\n Média geral de RMSE: {raiz_erro_quadratico_medio:.2f} px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1470bb-f18f-4977-887a-bcc6159b6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciação dos módulos da pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seg_module = U2NetSegmentationModule(\"./u2net_human_seg.pth\") #DeepLabSegmentationModule(device=device) \n",
    "kp_module = KeypointsModule()\n",
    "#reg_module = RegressionModule()\n",
    "\n",
    "feature_extractor = ExtractFeaturesKeypointsV2().to(device)\n",
    "# reg_module = XGBoostRegressionModule(\n",
    "#       model_dir = \"/content/multioutput_xgb_model.pkl\", \n",
    "#       scaler_dir = \"/content/treino_scaler.pkl\",\n",
    "# )\n",
    "reg_module =  DepthwiseSeparableConv1DRegressorModule(\n",
    "    model_dir = \"/content/separable_conv_regressor_weights.pt\",\n",
    "    scaler_dir = \"/content/treino_scaler.pkl\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020d13e-54f3-4cb2-a34e-e9d31e0fadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computação do RMSE para o conjunto de validação\n",
    "print_average_rmse_gpu(\n",
    "    loader = val_loader, \n",
    "    seg_module = seg_module, \n",
    "    kp_module = kp_module, \n",
    "    reg_module = reg_module, \n",
    "    feature_extractor_module = feature_extractor, \n",
    "    device = device,\n",
    "    salvar_mascaras=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b8183-b238-4c2e-a342-bbb92bdf8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computação do RMSE para o conjunto de teste\n",
    "print_average_rmse_gpu(\n",
    "    loader = test_loader, \n",
    "    seg_module = seg_module, \n",
    "    kp_module = kp_module, \n",
    "    reg_module = reg_module, \n",
    "    feature_extractor_module = feature_extractor, \n",
    "    device = device,\n",
    "    salvar_mascaras=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b044242-ca41-4349-95d3-e7f3c198dad2",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "\n",
    "\n",
    "| Method    | RMSE (px) validação | RMSE (px) teste |\n",
    "|-----------|---------------------|-----------------|\n",
    "| Baseline  | 136.28 pixels       |   134.42 pixels |   \n",
    "| U2Net+XGBoost  | 23.96 pixels               |      23.64 pixels            |\n",
    "| U2Net+DepthWiseConv1D  |  25.21 pixels               |       24.68 pixels           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05d4c8-4015-48d6-9e10-f8a400195af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
